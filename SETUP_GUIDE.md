# AI Summarization Platform - Complete Setup Guide

## üéØ **Dual Provider System**

Your AI Summarization Platform now supports **both cloud and local AI models**:

- **‚òÅÔ∏è Groq (Cloud)**: Fast, high-quality models via API
- **üñ•Ô∏è Ollama (Local)**: Privacy-focused, offline models on your machine

## üöÄ **Quick Start (3 Steps)**

### **1. Setup the Platform**
```bash
./setup.sh
```

### **2a. Setup Cloud Models (Groq)**
- Get your free API key from [Groq Console](https://console.groq.com/keys)
- Enter it in the web interface when you start the app

### **2b. Setup Local Models (Ollama)**
```bash
./setup_ollama.sh
```

### **3. Start the Application**
```bash
./start_app.sh
```

**üåê Access**: Open http://localhost:8000 in your browser

---

## üìã **Detailed Setup Instructions**

### **Option A: Cloud Models Only (Groq)**

**Pros**: ‚úÖ No local setup, ‚úÖ High performance, ‚úÖ Latest models
**Cons**: ‚ùå Requires internet, ‚ùå API costs (has free tier)

1. **Platform Setup**:
   ```bash
   ./setup.sh
   ```

2. **Get Groq API Key**:
   - Visit [console.groq.com/keys](https://console.groq.com/keys)
   - Sign up for free account
   - Generate API key

3. **Start & Configure**:
   ```bash
   ./start_app.sh
   ```
   - Enter API key in the web interface
   - Select from: Llama 3.1, Mixtral, Gemma models

---

### **Option B: Local Models Only (Ollama)**

**Pros**: ‚úÖ Complete privacy, ‚úÖ No API costs, ‚úÖ Offline operation
**Cons**: ‚ùå Requires good hardware, ‚ùå Slower inference, ‚ùå Large downloads

1. **Platform Setup**:
   ```bash
   ./setup.sh
   ```

2. **Ollama Setup**:
   ```bash
   ./setup_ollama.sh
   ```
   This will:
   - Install Ollama
   - Download recommended models
   - Start the service

3. **Start Application**:
   ```bash
   ./start_app.sh
   ```
   - Look for üñ•Ô∏è Local models in the interface

---

### **Option C: Both Providers (Recommended)**

**Best of both worlds**: Use cloud for speed, local for privacy

1. **Platform Setup**:
   ```bash
   ./setup.sh
   ```

2. **Setup Both Providers**:
   ```bash
   # Setup local models
   ./setup_ollama.sh
   
   # Get Groq API key from console.groq.com/keys
   ```

3. **Start & Configure**:
   ```bash
   ./start_app.sh
   ```
   - Enter Groq API key for cloud models
   - Local models will be auto-detected

---

## üñ•Ô∏è **System Requirements**

### **For Cloud Models (Groq)**
- ‚úÖ Any computer with internet
- ‚úÖ 1GB RAM minimum
- ‚úÖ Modern web browser

### **For Local Models (Ollama)**
- **Minimum**: 8GB RAM, 20GB storage
- **Recommended**: 16GB+ RAM, 50GB+ storage, SSD
- **Optimal**: 32GB+ RAM, GPU with 8GB+ VRAM

### **Model Size Guide**
```
Light Models (2-4GB):
‚îú‚îÄ‚îÄ phi3:mini (2.3GB) - Fast, good for testing
‚îú‚îÄ‚îÄ gemma:7b (4.8GB) - Balanced performance

Heavy Models (15-40GB):
‚îú‚îÄ‚îÄ llama3.1:70b (40GB) - Best quality, needs 64GB+ RAM
‚îú‚îÄ‚îÄ codellama:34b (19GB) - Code-focused, needs 32GB+ RAM
```

---

## üîß **Configuration Options**

### **Environment Variables (.env)**
```bash
# Cloud Provider
GROQ_API_KEY=your_groq_api_key_here

# Local Provider
OLLAMA_BASE_URL=http://localhost:11434

# Application
APP_NAME=AI Summarization Platform
HOST=0.0.0.0
PORT=8000
DEBUG=False
```

### **Model Selection Strategy**
- **Speed Priority**: Use Groq cloud models
- **Privacy Priority**: Use Ollama local models
- **Cost Priority**: Use Ollama (free after setup)
- **Quality Priority**: Use both and compare results

---

## üõ†Ô∏è **Management Commands**

### **Application Commands**
```bash
./setup.sh              # Initial platform setup
./start_app.sh           # Start the application
python run_app.py        # Alternative startup method
```

### **Ollama Commands**
```bash
ollama list              # List installed models
ollama pull llama3.1:8b  # Download a specific model
ollama run llama3.1:8b   # Test a model directly
ollama rm llama3.1:8b    # Remove a model to save space
ollama serve             # Start Ollama service manually
```

### **Development Commands**
```bash
# Test providers
python llm_providers.py

# Check Ollama status
curl http://localhost:11434/api/tags

# View application logs
# (logs appear in terminal where you run start_app.sh)
```

---

## üîç **Troubleshooting**

### **Groq Issues**
- ‚ùå "Invalid API key": Check key at console.groq.com
- ‚ùå "Rate limit": Wait or upgrade Groq plan
- ‚ùå "Network error": Check internet connection

### **Ollama Issues**
- ‚ùå "Ollama offline": Run `ollama serve` or `./setup_ollama.sh`
- ‚ùå "Model not found": Run `ollama pull <model_name>`
- ‚ùå "Out of memory": Try smaller models (phi3:mini)
- ‚ùå "Slow inference": Ensure sufficient RAM/GPU

### **Application Issues**
- ‚ùå "Port 8000 busy": App will auto-select another port
- ‚ùå "Module not found": Run `./setup.sh` again
- ‚ùå "No models available": Configure at least one provider

### **Performance Tips**
1. **For Ollama**: Close other applications to free RAM
2. **For Groq**: Use smaller models for faster responses
3. **For Both**: Compare results to find best model for your use case

---

## üìä **Model Comparison**

| Model | Provider | Size | Speed | Quality | Use Case |
|-------|----------|------|-------|---------|----------|
| Groq - Llama 3.1 8B | Cloud | API | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | General purpose |
| Groq - Mixtral 8x7B | Cloud | API | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Complex tasks |
| Ollama - Phi3 Mini | Local | 2.3GB | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | Testing/light use |
| Ollama - Llama 3.1 8B | Local | 4.7GB | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | Balanced local |
| Ollama - Llama 3.1 70B | Local | 40GB | üêå | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Best quality |

---

## üéâ **Success Indicators**

### **Platform Ready**
- ‚úÖ Web interface loads at http://localhost:8000
- ‚úÖ Professional UI with 3 columns visible
- ‚úÖ No error messages in terminal

### **Groq Working**
- ‚úÖ API key accepted (green checkmark)
- ‚úÖ ‚òÅÔ∏è Cloud models appear in interface
- ‚úÖ Fast summary generation (< 5 seconds)

### **Ollama Working**
- ‚úÖ Status shows "‚úÖ Online X models"
- ‚úÖ üñ•Ô∏è Local models appear in interface
- ‚úÖ Models respond (may take 10-30 seconds first time)

---

## üÜò **Getting Help**

### **Quick Diagnostics**
```bash
# Check if everything is running
curl http://localhost:8000/health
curl http://localhost:11434/api/tags

# Test providers
python llm_providers.py
```

### **Reset Everything**
```bash
# Reset platform
rm -rf ai_summarizer_env .env
./setup.sh

# Reset Ollama
ollama rm $(ollama list | tail -n +2 | awk '{print $1}')
./setup_ollama.sh
```

---

## üéØ **Next Steps**

1. **‚úÖ Choose your setup**: Cloud only, Local only, or Both
2. **‚úÖ Run setup scripts**: Follow the commands above
3. **‚úÖ Start application**: `./start_app.sh`
4. **‚úÖ Upload a document**: Test with PDF or sample text
5. **‚úÖ Compare models**: Try both cloud and local models
6. **‚úÖ Optimize settings**: Find the best models for your needs

**üöÄ You're now ready to use both cloud and local AI models for professional text summarization!**
